{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benedettoscala/ifttt-code-generator/blob/main/fine_tuning_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install transformers==4.36.2\n",
        "!pip install -U peft\n",
        "!pip install -U accelerate\n",
        "!pip install -U trl\n",
        "!pip install datasets==2.16.0\n",
        "!pip install sentencepiece\n",
        "!pip install -U bitsandbytes\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "0HueuUrz_TeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvpQWR5pwCGL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Loem6gt-xOVZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "secret_hf = userdata.get('HUGGINGFACE_TOKEN')\n",
        "!huggingface-cli login --token $secret_hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzQkARHFU2Hl"
      },
      "source": [
        "# Preprocessing dei dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7_T1dEsXfWS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/benedettoscala/ifttt-code-generator\n",
        "%cd ifttt-code-generator/\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")"
      ],
      "metadata": {
        "id": "_SWl39j5-3iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "#Configurazione modello base e quantizzazione\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "#Caricamento del modello e tokenizer\n",
        "print(\"Caricamento del modello base...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(\"Caricamento del tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Prepara il modello per k-bit training (disabilita gradienti su pesi int4 ecc.)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 3. Configurazione LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]\n",
        "    # i moduli dei transformer li ho controllati su hugging face, sono questi (godo)\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "26hh2jaO-5pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caricamento e pulizia del dataset\n",
        "csv_path = \"datasets/cleaned_and_combined.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "df.dropna(subset=[\"cleaned_description\", \"filter_code\"], inplace=True)\n",
        "\n",
        "#drop duplicates\n",
        "df.drop_duplicates(subset=[\"cleaned_description\", \"filter_code\"], inplace=True)\n",
        "\n",
        "#Suddivisione train   e val\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset  = Dataset.from_pandas(eval_df)"
      ],
      "metadata": {
        "id": "ey3NStBN_Bhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "uboU1hT5KTYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "# Funzione di tokenizzazione\n",
        "def tokenize_function(examples):\n",
        "    separator = \"\\n###\\n\"\n",
        "\n",
        "    # Concateno desc + code\n",
        "    full_text = [\n",
        "        desc + separator + code\n",
        "        for desc, code in zip(examples[\"cleaned_description\"], examples[\"filter_code\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizza con padding e truncation \"coerenti\"\n",
        "    tokenized = tokenizer(\n",
        "        full_text,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"  # cosi ottengo shape costanti\n",
        "    )\n",
        "\n",
        "    # Calcolo la lunghezza del prompt con gli stessi identici parametri\n",
        "    prompt_text = [\n",
        "        desc + separator\n",
        "        for desc in examples[\"cleaned_description\"]\n",
        "    ]\n",
        "    tokenized_prompt = tokenizer(\n",
        "        prompt_text,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"  # stesse impostazioni\n",
        "    )\n",
        "    prompt_lengths = [\n",
        "        sum(p_id != tokenizer.pad_token_id for p_id in p_ids)\n",
        "        for p_ids in tokenized_prompt[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    # Costruisco le label: maschero la parte del prompt con -100\n",
        "    labels = []\n",
        "    for i, seq in enumerate(tokenized[\"input_ids\"]):\n",
        "        prompt_len = prompt_lengths[i]\n",
        "        # Il prompt è su N token e la rimanente parte su (512 - N) token\n",
        "        # ATTENZIONE: se usi \"padding=max_length\" la seq avrà sempre lunghezza 512\n",
        "        # e i token in eccesso (se esiste) saranno solo pad.\n",
        "        # Esempio: seq[prompt_len:] serve a prendere la parte del \"code\".\n",
        "        masked_labels = [-100]*prompt_len + seq[prompt_len:]\n",
        "        labels.append(masked_labels)\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset  = eval_dataset.map(tokenize_function,  batched=True)\n",
        "\n",
        "# Data collator con padding a sinistra\n",
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding='longest'  # la dimensione massima del batch la decide a runtime\n",
        ")"
      ],
      "metadata": {
        "id": "iOCFKwkm_Jyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "vyRtjlC-MEcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Trainer\n",
        "from datasets import load_metric\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Impostazioni di training specifiche per Seq2Seq\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    save_steps=100,\n",
        "    logging_steps=2,\n",
        "    eval_strategy=\"epoch\",  # Puoi anche usare \"epoch\"\n",
        "    load_best_model_at_end=False,\n",
        "    save_total_limit=3,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    report_to=\"wandb\",\n",
        "    #predict_with_generate=True  # Abilitare la generazione delle predizioni\n",
        ")\n",
        "\n",
        "# Funzione per il calcolo delle metriche\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Verifica il tipo e il range dei token ID\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    # Assicurati che le predizioni siano in formato numpy array\n",
        "    if isinstance(predictions, torch.Tensor):\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "    # Verifica se le predizioni contengono logits o token ID\n",
        "    if predictions.dtype not in [int, 'int32', 'int64']:\n",
        "        # Se sono logits, converti in token ID utilizzando argmax\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    # Decodifica le predizioni e le etichette in testo\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU\n",
        "    bleu_scores = [\n",
        "        sentence_bleu([ref.split()], pred.split(), smoothing_function=SmoothingFunction().method1)\n",
        "        for pred, ref in zip(decoded_preds, decoded_labels)\n",
        "    ]\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    # METEOR\n",
        "    # METEOR\n",
        "    meteor_scores = [\n",
        "        meteor_score([ref.split()], pred.split())\n",
        "        for pred, ref in zip(decoded_preds, decoded_labels)\n",
        "    ]\n",
        "\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "\n",
        "    # ROUGE\n",
        "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores = [\n",
        "        rouge.score(ref, pred)\n",
        "        for pred, ref in zip(decoded_preds, decoded_labels)\n",
        "    ]\n",
        "    avg_rouge1 = sum(score[\"rouge1\"].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "    avg_rouge2 = sum(score[\"rouge2\"].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "    avg_rougeL = sum(score[\"rougeL\"].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "    return {\n",
        "        \"bleu\": avg_bleu,\n",
        "        \"meteor\": avg_meteor,\n",
        "        \"rouge1\": avg_rouge1,\n",
        "        \"rouge2\": avg_rouge2,\n",
        "        \"rougeL\": avg_rougeL,\n",
        "    }\n",
        "\n",
        "# Creazione Seq2SeqTrainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Avvia il training\n",
        "trainer.train()\n",
        "\n",
        "# Salvataggio finale LoRA + quantization\n",
        "trainer.save_model(\"./results/best_model\")\n"
      ],
      "metadata": {
        "id": "A1A5paYQ_N7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lC5wYhh2NTum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: zip the results and upload it on drive\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Specify the directory to zip\n",
        "directory_to_zip = \"/content/ifttt-code-generator\"  # Replace with the actual directory\n",
        "\n",
        "# Specify the zip file name\n",
        "zip_file_name = \"/content/drive/MyDrive/ifttt_results.zip\"\n",
        "\n",
        "# Create a zip archive\n",
        "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(directory_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, arcname=os.path.relpath(file_path, directory_to_zip))\n",
        "\n",
        "print(f\"Successfully zipped the contents of '{directory_to_zip}' to '{zip_file_name}'\")"
      ],
      "metadata": {
        "id": "7Db71rkpNo_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "# Percorso del modello fine-tunato\n",
        "finetuned_model_path = \"./results/best_model\"\n",
        "basemodel_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# Caricamento del modello base\n",
        "\n",
        "\n",
        "#create offload directory if it doesn't exist\n",
        "if not os.path.exists(\"./offload\"):\n",
        "    os.makedirs(\"./offload\")\n",
        "\n",
        "\n",
        "# Caricamento del modello e del tokenizer fine-tunati\n",
        "print(\"Caricamento del modello fine-tunato...\")\n",
        "from peft import PeftModel\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    basemodel_path,\n",
        "    torch_dtype=torch.float16,         # or torch.bfloat16, depending on your setup\n",
        "    quantization_config=bnb_config,           # 4-bit quantization\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"./offload\"         # <= Provide a folder path\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    finetuned_model_path,\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n"
      ],
      "metadata": {
        "id": "Gt6wRWM9Ip2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Funzione per generare il codice IFTTT\n",
        "def generate_ifttt_code(prompt, max_length=512, num_return_sequences=1):\n",
        "    # Tokenizzazione dell'input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generazione del codice\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "    # Decodifica del risultato\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return decoded_outputs\n",
        "\n",
        "# Prompt per generare il codice IFTTT\n",
        "prompt = \"if the current hour is 17, send a tweet\"\n",
        "generated_code = generate_ifttt_code(prompt)\n",
        "\n",
        "# Stampa del codice generato\n",
        "print(\"\\nCodice IFTTT generato:\")\n",
        "for i, code in enumerate(generated_code, 1):\n",
        "    print(f\"{code}\")\n"
      ],
      "metadata": {
        "id": "nr950ogTIl_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "goKpo98jNTY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}