{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benedettoscala/ifttt-code-generator/blob/main/bart_nl2ifttt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup\n",
        "In this section, we install the necessary dependencies for the notebook. The libraries `evaluate` and `rouge_score` are installed to facilitate text evaluation, particularly for computing ROUGE scores, which are commonly used for assessing the quality of text generation models.\n",
        "\n"
      ],
      "metadata": {
        "id": "YGS9ZEtONUh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmyuqPXtDOv5",
        "outputId": "99f3e8dd-c80e-4398-a101-272e7c6c55c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Repository Cloning and Setup\n",
        "We clone the `ifttt-code-generator` repository from GitHub, which contains the required code for this project. After cloning, we navigate into the repository directory and pull the latest changes to ensure we have the most up-to-date version."
      ],
      "metadata": {
        "id": "YoLwrVmoNXGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/benedettoscala/ifttt-code-generator\n",
        "%cd ifttt-code-generator/\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLpELkCb5Yez",
        "outputId": "5edf26ef-f669-4dc9-81ff-3a57ba134186"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ifttt-code-generator'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 122 (delta 66), reused 25 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (122/122), 14.63 MiB | 18.20 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "/content/ifttt-code-generator/ifttt-code-generator\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Processing and Tokenization\n",
        "This section of the notebook performs dataset loading, cleaning, and tokenization using the `facebook/bart-large` tokenizer.\n",
        "\n",
        "- First, the necessary libraries (`pandas`, `numpy`, `matplotlib.pyplot`, and `transformers`) are imported.\n",
        "- The `facebook/bart-large` tokenizer is loaded, and if no padding token is defined, it is set to the EOS token.\n",
        "- A dataset is loaded from a CSV file (`datasets/cleaned_and_combined.csv`), and data cleaning is applied by removing missing values and duplicate entries in the relevant columns.\n",
        "- Each sample in the dataset is tokenized separately for the description, code, and the combined text using a separator (`\\n###\\n`).\n",
        "- Token length statistics (min, max, mean, and median) are computed for the description, code, and full text.\n",
        "- Finally, a histogram is plotted to visualize the distribution of token lengths in the dataset, with a reference line at 256 tokens to help assess token length constraints.\n"
      ],
      "metadata": {
        "id": "ZmYmiuKWNZPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Imposta il checkpoint del modello (BART-large)\n",
        "model_checkpoint = \"facebook/bart-large\"\n",
        "\n",
        "# Carica il tokenizer per facebook/bart-large\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# Se il tokenizer non ha un token di padding definito, impostalo uguale al token EOS\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Carica il dataset\n",
        "csv_path = \"datasets/cleaned_and_combined.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Rimuovi righe con valori mancanti e duplicati nelle colonne di interesse\n",
        "df.dropna(subset=[\"cleaned_description\", \"filter_code\"], inplace=True)\n",
        "df.drop_duplicates(subset=[\"cleaned_description\", \"filter_code\"], inplace=True)\n",
        "\n",
        "# Definisci un separatore per unire descrizione e codice\n",
        "separator = \"\\n###\\n\"\n",
        "\n",
        "# Liste per salvare le lunghezze in token\n",
        "description_lengths = []\n",
        "code_lengths = []\n",
        "combined_lengths = []\n",
        "\n",
        "# Itera su ogni esempio nel dataset\n",
        "for _, row in df.iterrows():\n",
        "    description = row[\"cleaned_description\"]\n",
        "    code = row[\"filter_code\"]\n",
        "\n",
        "    # Tokenizza la descrizione senza troncamento\n",
        "    desc_tokens = tokenizer.encode(description, truncation=False)\n",
        "    # Tokenizza il codice senza troncamento\n",
        "    code_tokens = tokenizer.encode(code, truncation=False)\n",
        "    # Tokenizza la concatenazione: descrizione + separatore + codice\n",
        "    combined_text = description + separator + code\n",
        "    combined_tokens = tokenizer.encode(combined_text, truncation=False)\n",
        "\n",
        "    # Salva le lunghezze\n",
        "    description_lengths.append(len(desc_tokens))\n",
        "    code_lengths.append(len(code_tokens))\n",
        "    combined_lengths.append(len(combined_tokens))\n",
        "\n",
        "# Funzione per stampare statistiche (min, max, media, mediana)\n",
        "def print_stats(name, lengths):\n",
        "    print(f\"Statistiche per {name}:\")\n",
        "    print(\"  Min:\", np.min(lengths))\n",
        "    print(\"  Max:\", np.max(lengths))\n",
        "    print(\"  Media:\", np.mean(lengths))\n",
        "    print(\"  Mediana:\", np.median(lengths))\n",
        "    print()\n",
        "\n",
        "print_stats(\"la descrizione\", description_lengths)\n",
        "print_stats(\"il codice\", code_lengths)\n",
        "print_stats(\"il testo completo (descrizione + codice)\", combined_lengths)\n",
        "\n",
        "# Visualizza la distribuzione della lunghezza in token del testo completo\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(combined_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(256, color='red', linestyle='dashed', linewidth=2, label=\"256 token\")\n",
        "plt.title(\"Distribuzione della lunghezza dei testi (in token) per facebook/bart-large\")\n",
        "plt.xlabel(\"Numero di token\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-yM4u9v5KQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123qky9TDOv7"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "RrEpZTnvDOv8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from datasets import load_dataset,load_from_disk\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXxDkkOnDOv8"
      },
      "source": [
        "### Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG7EmiWxDOv8"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "F_0RpXuE52Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading and Splitting\n",
        "This section of the notebook loads and processes the dataset for training and evaluation.\n",
        "\n",
        "- The dataset is loaded from a CSV file using `pandas`.\n",
        "- Data cleaning is applied by removing missing values and duplicate entries in the `cleaned_description` and `filter_code` columns.\n",
        "- The dataset is then split into training (80%) and testing (20%) subsets using `train_test_split`, ensuring reproducibility with a fixed random seed (`random_state=42`).\n",
        "- The `pandas` DataFrames are converted into Hugging Face `Dataset` objects for better compatibility with NLP models.\n",
        "- Finally, the dataset is structured into a `DatasetDict`, which organizes the training and testing sets for further processing.\n",
        "\n",
        "The number of examples in the training and test sets is printed at the end to confirm the split.\n"
      ],
      "metadata": {
        "id": "3epzzMzANcVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZRG7y2MDOv9"
      },
      "outputs": [],
      "source": [
        "base_path = os.getcwd()\n",
        "absolute_path = os.path.join(base_path,r'ifttt-code-generator/datasets/cleaned_and_combined.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "tsnlWoZFDOv9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Caricamento del dataset\n",
        "csv_path = \"ifttt-code-generatordatasets/cleaned_and_combined.csv\"\n",
        "df = pd.read_csv(absolute_path)\n",
        "\n",
        "#droppa i duplicati e i valori nulli se ci sono\n",
        "df.dropna(subset=[\"cleaned_description\", \"filter_code\"], inplace=True)\n",
        "df.drop_duplicates(subset=[\"cleaned_description\", \"filter_code\"], inplace=True)\n",
        "\n",
        "# Suddivisione in train e test set (80%-20%)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convertiamo i DataFrame in Dataset Hugging Face\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "# Stampa delle dimensioni dei set di training e test\n",
        "print(\"Train set size:\", len(dataset[\"train\"]))\n",
        "print(\"Test set size:\", len(dataset[\"test\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "lYGefEPVDOv9"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"facebook/bart-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "zchbLUnLDOv9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4NH2VuGDOv9"
      },
      "source": [
        "### Model Tokenization and Preprocessing\n",
        "In this section, we define the tokenizer and preprocess the dataset for training.\n",
        "\n",
        "- The `facebook/bart-large` tokenizer is loaded using `AutoTokenizer.from_pretrained`.\n",
        "- We define the maximum input length (256 tokens) and target length (128 tokens) to ensure that text sequences fit within the model's constraints.\n",
        "- A prefix (`ifttt_prompt: `) is added to each description to provide context for the model.\n",
        "- The `preprocess_function` tokenizes the descriptions (`cleaned_description`) and corresponding code snippets (`filter_code`), ensuring that they respect the defined token limits and truncation settings.\n",
        "- The labels (target sequences) are tokenized separately, and their token IDs are stored within the model input structure.\n",
        "- Finally, the preprocessing function is applied to the dataset using `.map()` to tokenize all data efficiently.\n",
        "\n",
        "The progress bars at the bottom confirm that the dataset has been successfully tokenized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "LPCle05DDOv-"
      },
      "outputs": [],
      "source": [
        "max_input_length = 256\n",
        "max_target_length = 256\n",
        "prefix1 = \"ifttt_prompt: \"\n",
        "prefix2 = \" ifttt_context: \"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer([prefix1 + prompt for prompt in examples['cleaned_description']],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples['filter_code'], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "qWhBw7IeDOv-"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDOJKqGODOv-"
      },
      "source": [
        "### Evaluation Metrics Setup\n",
        "In this section, we load three commonly used evaluation metrics for text generation tasks using the `evaluate` library:\n",
        "\n",
        "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Measures overlap between generated text and reference text based on n-grams and longest common subsequence. It is widely used for summarization tasks.\n",
        "- **BLEU (Bilingual Evaluation Understudy):** Computes precision-based similarity by comparing generated text with reference translations. It is commonly used in machine translation.\n",
        "- **METEOR (Metric for Evaluation of Translation with Explicit ORdering):** Improves upon BLEU by considering synonym matching, stemming, and word order to provide a more nuanced evaluation.\n",
        "\n",
        "These metrics will be used to assess the quality of model-generated text by comparing it with ground-truth references.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "dEf8DusDDOv-"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")\n",
        "bleu_score = evaluate.load(\"bleu\")\n",
        "meteor_score = evaluate.load(\"meteor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3hUEYHODOv_"
      },
      "source": [
        "### Model Initialization\n",
        "In this section, we load and configure the model for sequence-to-sequence learning.\n",
        "\n",
        "- The `facebook/bart-large` model is loaded using `AutoModelForSeq2SeqLM.from_pretrained()`, which retrieves a pre-trained sequence-to-sequence model.\n",
        "- The generation parameters are set:\n",
        "  - `max_new_tokens = 128`: The model can generate up to 128 tokens per output.\n",
        "  - `min_new_tokens = 5`: The model must generate at least 5 tokens.\n",
        "- These settings are applied both through `generation_config` and `config` to ensure consistency.\n",
        "\n",
        "Once the model is loaded, we can use it for text generation tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "-_t1nl5ZDOv_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "q9QIoSnIDOv_"
      },
      "outputs": [],
      "source": [
        "model.generation_config.max_new_tokens = 256\n",
        "model.generation_config.min_new_tokens = 5\n",
        "model.config.max_new_tokens = 256\n",
        "model.config.min_new_tokens = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "FfkKSghSDOv_"
      },
      "outputs": [],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "kU5n-2NMDOv_"
      },
      "outputs": [],
      "source": [
        "print(model.generation_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tO0tygyDOv_"
      },
      "source": [
        "\n",
        "### Data Collator\n",
        "This section prepares the data for training by using a data collator.\n",
        "\n",
        "- `DataCollatorForSeq2Seq` is initialized with the tokenizer and model to dynamically pad inputs to the longest sequence in a batch, improving computational efficiency.\n",
        "- Unnecessary columns from the tokenized dataset are removed to ensure compatibility with the model.\n",
        "- A sample batch of two training examples is processed through the data collator, demonstrating the transformation into tensor format.\n",
        "\n",
        "The data collator ensures that input sequences are properly formatted and padded for efficient batch processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "IJxfCBaMDOv_"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "1VFfkqUNDOv_"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "-msE9xhWDOv_"
      },
      "outputs": [],
      "source": [
        "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
        "data_collator(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_87c9chDOwA"
      },
      "source": [
        "#### Post Processing for ROUGE computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "a8anZrBlDOwA"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "JVbxqJ5lJzMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6DA_JmzDOwA"
      },
      "source": [
        "### Training Setup with Seq2SeqTrainer\n",
        "This section sets up and starts the fine-tuning process using `Seq2SeqTrainer`, a high-level API for training sequence-to-sequence models.\n",
        "\n",
        "#### **Evaluation Metrics**\n",
        "- The `ROUGE`, `BLEU`, and `METEOR` scores are loaded using the `evaluate` library.\n",
        "- The `compute_metrics` function is defined to:\n",
        "  - Replace `-100` values in labels (used to ignore padding during training) with the tokenizer's pad token ID.\n",
        "  - Decode the modelâ€™s predictions and reference labels into human-readable text.\n",
        "  - Postprocess and compute evaluation metrics.\n",
        "  - Normalize and return the computed scores.\n",
        "\n",
        "#### **Training Arguments**\n",
        "- The `Seq2SeqTrainingArguments` class is used to define training parameters:\n",
        "  - The output directory is set to `\"nl2sql_bart\"`.\n",
        "  - Training, evaluation, and model checkpointing occur at the end of each epoch.\n",
        "  - A batch size of 8 is used for both training and evaluation.\n",
        "  - The total number of training epochs is set to `30`.\n",
        "  - The model saves only the last two checkpoints to save storage.\n",
        "  - If a GPU is available, mixed-precision training (`fp16=True`) is enabled for efficiency.\n",
        "  - `predict_with_generate=True` ensures that model predictions are generated instead of just computing loss.\n",
        "\n",
        "#### **Trainer Initialization**\n",
        "- A `Seq2SeqTrainer` instance is created using:\n",
        "  - The model to be trained.\n",
        "  - The training and evaluation datasets.\n",
        "  - A `data_collator` for dynamic padding.\n",
        "  - The `compute_metrics` function for evaluation.\n",
        "\n",
        "#### **Training Execution**\n",
        "- The training is started using `trainer.train()`, which fine-tunes the model on the dataset.\n",
        "\n",
        "This setup ensures an efficient and automated training process with built-in evaluation and checkpointing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "uodurTu0DOwA"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Carica le metriche\n",
        "rouge_score = evaluate.load(\"rouge\")\n",
        "bleu_score = evaluate.load(\"bleu\")\n",
        "meteor_score = evaluate.load(\"meteor\")\n",
        "\n",
        "num_train_epochs = 30\n",
        "\n",
        "# Funzione per la generazione e la valutazione delle metriche\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Rimpiazza -100 nei labels con tokenizer.pad_token_id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decodifica delle predizioni e delle etichette\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rimozione di spazi inutili\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    # Calcolo delle metriche\n",
        "    rouge_results = rouge_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    bleu_results = bleu_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    meteor_results = meteor_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Normalizza i valori delle metriche\n",
        "    rouge_results = {k: round(v * 100, 4) for k, v in rouge_results.items()}\n",
        "    bleu_results = round(bleu_results[\"bleu\"] * 100, 2)\n",
        "    meteor_results = round(meteor_results[\"meteor\"] * 100, 2)\n",
        "\n",
        "    return {\n",
        "        **rouge_results,\n",
        "        \"bleu\": bleu_results,\n",
        "        \"meteor\": meteor_results\n",
        "    }\n",
        "\n",
        "# Definizione degli argomenti di training\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"nl2sql_bart\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",  # Valutazione dopo ogni epoca\n",
        "    save_strategy=\"epoch\",  # Salvataggio automatico ogni epoca\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    fp16=torch.cuda.is_available(),  # Usa FP16 se disponibile\n",
        "    report_to=\"none\",  # Evita di inviare log a sistemi di tracking\n",
        "    predict_with_generate=True\n",
        ")\n",
        "\n",
        "# Creazione del Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Avvio del training\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference with Text-to-Text Pipeline\n",
        "In this section, we load the trained model and use it for text generation.\n",
        "\n",
        "#### **Model Loading**\n",
        "- The trained model is loaded from the specified checkpoint directory (`nl2sql_epoch30`).\n",
        "- The `pipeline` function from `transformers` is used to create a text-to-text generation pipeline.\n",
        "- Both the model and tokenizer are loaded from the same checkpoint to ensure compatibility.\n",
        "\n",
        "#### **Generating Predictions**\n",
        "- A sample prompt is provided to the model:  \n",
        "  `\"ifttt prompt: Create an applet that saves new photos from my phone to a Google Drive folder automatically\"`\n",
        "- The model generates a text-based response using `pipeline(\"text2text-generation\")`.\n",
        "- The generated output is limited to a maximum of 128 tokens.\n",
        "\n",
        "#### **Usage**\n",
        "- This pipeline allows the model to generate structured text based on natural language prompts.\n",
        "- It can be used to create IFTTT-like (If This Then That) automation rules based on textual descriptions.\n",
        "\n",
        "This setup enables efficient inference, allowing the model to process user inputs and generate corresponding automation rules.\n"
      ],
      "metadata": {
        "id": "U5jE2a89OI-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your shared folder in Google Drive\n",
        "shared_folder_path = \"/content/drive/Shareddrives/NLPMODELS\"  # Replace with your actual shared drive and folder name\n",
        "\n",
        "# Create the shared folder if it doesn't exist\n",
        "!mkdir -p \"{shared_folder_path}\"\n",
        "\n",
        "# Save the nl2sql_bart folder to your shared Google Drive folder\n",
        "!cp -r nl2sql_bart \"{shared_folder_path}\""
      ],
      "metadata": {
        "id": "FsKFLc_p23fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (after training) -  Load from Google Drive shared folder\n",
        "model_path = f\"{shared_folder_path}/nl2sql_bart/checkpoint-1020\" # Or the appropriate checkpoint\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=model_path, tokenizer=model_checkpoint)\n",
        "\n",
        "# Example usage\n",
        "prompt = \"ifttt prompt: Create an applet that turns on hue lights from 7pm to 9pm\"\n",
        "result = generator(prompt, max_length=256)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "pGisnKVZ62ZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4866003,
          "sourceId": 8211052,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}